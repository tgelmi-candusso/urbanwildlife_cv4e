# Here's where you define experiment-specific hyperparameters.
# You can also create lists and group parameters together into nested sub-parts.
# In Python, this is all read as a dict.

# environment/computational parameters
seed: 32678456782 # random number generator seed (long integer value)
device: cuda
num_workers: 6

# dataset parameters
data_root: "/home/mgonzalez3/data"
num_classes: 62 # change back to 52

# training hyperparameters
image_size: [224, 224]
num_epochs: 50
batch_size: 128 ##it was 1 with random, tried 32 bs way faster tried 128 and only 10sec faster per epoc
learning_rate: 0.001
weight_decay: 0.001

log_dir: "logs/croppedOutputTest" #or split_by_loc split_across_loc or random_split (already ran)
split_type: "split_random" #or split_across_loc or split_random
model_dir: "model_states/croppedOutputTest"

##data augmentation
#curriculum learning, max numb of images per class
# 62 instances of num reflects 62 classes, num species trainng model on
# gradually learn
#max_num: [999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999]
max_num: [100, 100, 500, 1000, 20000]
#loss weights
weights:
  [
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
  ]
